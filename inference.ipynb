{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96cacd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chengxin/chengxin/anaconda3/envs/vaflow/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at /home/chengxin/chengxin/vasflow/model/stable_audio/ckpt/transformer_text were not used when initializing StableAudioDiTModel: ['transformer_blocks.23.attn1.to_k.weight', 'transformer_blocks.23.norm3.weight', 'transformer_blocks.21.norm2.weight', 'transformer_blocks.19.attn2.to_q.weight', 'transformer_blocks.21.attn1.to_k.weight', 'transformer_blocks.16.norm1.bias', 'transformer_blocks.11.norm2.weight', 'transformer_blocks.20.norm2.weight', 'transformer_blocks.21.norm3.weight', 'transformer_blocks.23.norm1.weight', 'transformer_blocks.22.ff.net.2.weight', 'transformer_blocks.23.attn1.to_out.0.weight', 'transformer_blocks.23.attn2.to_v.weight', 'transformer_blocks.11.norm3.weight', 'transformer_blocks.16.attn1.to_out.0.weight', 'transformer_blocks.14.attn1.to_q.weight', 'transformer_blocks.19.ff.net.0.proj.weight', 'transformer_blocks.14.attn2.to_v.weight', 'transformer_blocks.14.norm3.bias', 'transformer_blocks.18.norm3.weight', 'transformer_blocks.21.ff.net.2.weight', 'transformer_blocks.14.attn1.to_out.0.weight', 'transformer_blocks.18.norm2.weight', 'transformer_blocks.20.norm1.bias', 'transformer_blocks.10.ff.net.0.proj.weight', 'transformer_blocks.16.attn1.to_q.weight', 'transformer_blocks.10.norm3.bias', 'transformer_blocks.18.attn1.to_k.weight', 'transformer_blocks.10.norm3.weight', 'transformer_blocks.21.attn1.to_out.0.weight', 'transformer_blocks.18.ff.net.0.proj.bias', 'transformer_blocks.20.norm1.weight', 'transformer_blocks.16.ff.net.0.proj.bias', 'transformer_blocks.17.ff.net.0.proj.weight', 'transformer_blocks.13.ff.net.0.proj.weight', 'transformer_blocks.10.ff.net.2.weight', 'transformer_blocks.17.attn1.to_out.0.weight', 'transformer_blocks.13.norm1.weight', 'transformer_blocks.16.norm3.bias', 'transformer_blocks.15.attn2.to_q.weight', 'cross_attention_proj.2.weight', 'transformer_blocks.11.attn2.to_q.weight', 'transformer_blocks.14.attn2.to_out.0.weight', 'transformer_blocks.12.ff.net.2.weight', 'transformer_blocks.14.ff.net.2.bias', 'transformer_blocks.16.attn1.to_k.weight', 'transformer_blocks.19.attn2.to_k.weight', 'cross_attention_proj.0.weight', 'transformer_blocks.11.attn1.to_q.weight', 'transformer_blocks.17.ff.net.2.weight', 'transformer_blocks.21.norm1.weight', 'transformer_blocks.22.ff.net.0.proj.bias', 'transformer_blocks.10.attn2.to_v.weight', 'transformer_blocks.19.norm3.bias', 'transformer_blocks.19.norm2.bias', 'transformer_blocks.10.attn1.to_k.weight', 'transformer_blocks.18.ff.net.0.proj.weight', 'transformer_blocks.11.attn2.to_out.0.weight', 'transformer_blocks.15.ff.net.0.proj.bias', 'transformer_blocks.20.attn2.to_out.0.weight', 'transformer_blocks.17.ff.net.2.bias', 'transformer_blocks.22.ff.net.0.proj.weight', 'transformer_blocks.23.ff.net.0.proj.bias', 'transformer_blocks.13.norm3.bias', 'transformer_blocks.13.attn1.to_v.weight', 'transformer_blocks.14.norm1.weight', 'transformer_blocks.19.attn1.to_v.weight', 'transformer_blocks.21.norm2.bias', 'transformer_blocks.12.norm2.bias', 'transformer_blocks.11.attn1.to_out.0.weight', 'transformer_blocks.18.attn2.to_out.0.weight', 'transformer_blocks.14.attn1.to_v.weight', 'transformer_blocks.22.attn2.to_q.weight', 'transformer_blocks.23.norm2.bias', 'transformer_blocks.21.attn1.to_q.weight', 'transformer_blocks.15.attn1.to_q.weight', 'transformer_blocks.19.attn2.to_out.0.weight', 'transformer_blocks.22.attn1.to_out.0.weight', 'transformer_blocks.23.norm3.bias', 'transformer_blocks.11.attn1.to_k.weight', 'transformer_blocks.15.attn1.to_v.weight', 'transformer_blocks.17.attn1.to_q.weight', 'transformer_blocks.16.attn2.to_k.weight', 'transformer_blocks.23.attn2.to_q.weight', 'transformer_blocks.16.attn2.to_v.weight', 'transformer_blocks.20.norm3.weight', 'transformer_blocks.13.attn1.to_q.weight', 'transformer_blocks.13.attn2.to_k.weight', 'transformer_blocks.13.attn1.to_out.0.weight', 'transformer_blocks.12.attn1.to_v.weight', 'transformer_blocks.16.ff.net.2.weight', 'transformer_blocks.18.attn1.to_q.weight', 'transformer_blocks.11.norm1.weight', 'transformer_blocks.23.norm1.bias', 'transformer_blocks.17.attn2.to_out.0.weight', 'transformer_blocks.23.ff.net.2.bias', 'transformer_blocks.17.norm3.bias', 'transformer_blocks.22.norm2.weight', 'transformer_blocks.23.ff.net.0.proj.weight', 'transformer_blocks.21.attn1.to_v.weight', 'transformer_blocks.12.attn2.to_k.weight', 'transformer_blocks.20.attn2.to_q.weight', 'transformer_blocks.11.attn2.to_k.weight', 'transformer_blocks.10.ff.net.2.bias', 'transformer_blocks.23.ff.net.2.weight', 'transformer_blocks.20.attn1.to_out.0.weight', 'transformer_blocks.15.norm1.bias', 'transformer_blocks.17.attn2.to_k.weight', 'transformer_blocks.10.attn1.to_q.weight', 'transformer_blocks.23.attn2.to_k.weight', 'transformer_blocks.18.attn2.to_q.weight', 'transformer_blocks.20.attn1.to_v.weight', 'transformer_blocks.22.norm1.weight', 'transformer_blocks.12.norm1.weight', 'transformer_blocks.12.ff.net.2.bias', 'transformer_blocks.21.ff.net.0.proj.weight', 'transformer_blocks.22.attn2.to_v.weight', 'transformer_blocks.13.norm3.weight', 'transformer_blocks.12.norm3.weight', 'transformer_blocks.19.ff.net.2.weight', 'transformer_blocks.22.norm3.weight', 'transformer_blocks.14.attn1.to_k.weight', 'transformer_blocks.16.attn2.to_q.weight', 'transformer_blocks.18.attn2.to_k.weight', 'transformer_blocks.14.attn2.to_k.weight', 'transformer_blocks.20.ff.net.0.proj.weight', 'transformer_blocks.21.norm3.bias', 'transformer_blocks.13.ff.net.0.proj.bias', 'transformer_blocks.16.ff.net.2.bias', 'transformer_blocks.20.attn1.to_q.weight', 'transformer_blocks.19.attn1.to_q.weight', 'transformer_blocks.16.norm1.weight', 'transformer_blocks.13.attn2.to_v.weight', 'transformer_blocks.12.norm2.weight', 'transformer_blocks.14.ff.net.0.proj.weight', 'transformer_blocks.20.attn2.to_k.weight', 'transformer_blocks.10.norm2.bias', 'transformer_blocks.17.norm2.bias', 'transformer_blocks.22.ff.net.2.bias', 'transformer_blocks.10.attn2.to_k.weight', 'transformer_blocks.11.ff.net.2.bias', 'transformer_blocks.16.norm3.weight', 'transformer_blocks.20.attn1.to_k.weight', 'transformer_blocks.19.ff.net.2.bias', 'transformer_blocks.19.attn1.to_out.0.weight', 'transformer_blocks.13.attn1.to_k.weight', 'transformer_blocks.22.attn2.to_out.0.weight', 'transformer_blocks.10.attn1.to_out.0.weight', 'transformer_blocks.16.ff.net.0.proj.weight', 'transformer_blocks.13.norm2.weight', 'transformer_blocks.22.attn1.to_k.weight', 'transformer_blocks.10.attn2.to_out.0.weight', 'transformer_blocks.22.attn1.to_v.weight', 'transformer_blocks.18.attn1.to_out.0.weight', 'transformer_blocks.12.norm1.bias', 'transformer_blocks.11.ff.net.0.proj.bias', 'transformer_blocks.16.norm2.bias', 'transformer_blocks.21.attn2.to_k.weight', 'transformer_blocks.19.ff.net.0.proj.bias', 'transformer_blocks.14.ff.net.2.weight', 'transformer_blocks.13.attn2.to_out.0.weight', 'transformer_blocks.19.norm1.weight', 'transformer_blocks.23.attn2.to_out.0.weight', 'transformer_blocks.15.attn2.to_v.weight', 'transformer_blocks.15.ff.net.2.bias', 'transformer_blocks.10.ff.net.0.proj.bias', 'transformer_blocks.20.norm2.bias', 'transformer_blocks.16.attn2.to_out.0.weight', 'transformer_blocks.22.attn1.to_q.weight', 'transformer_blocks.10.norm1.bias', 'transformer_blocks.20.norm3.bias', 'transformer_blocks.17.attn1.to_k.weight', 'transformer_blocks.10.norm2.weight', 'transformer_blocks.19.attn2.to_v.weight', 'transformer_blocks.15.norm1.weight', 'transformer_blocks.17.norm3.weight', 'transformer_blocks.19.norm2.weight', 'transformer_blocks.15.attn1.to_k.weight', 'transformer_blocks.22.norm3.bias', 'transformer_blocks.15.attn2.to_k.weight', 'transformer_blocks.18.norm1.bias', 'transformer_blocks.22.norm2.bias', 'transformer_blocks.13.norm2.bias', 'transformer_blocks.15.norm3.weight', 'transformer_blocks.21.attn2.to_out.0.weight', 'transformer_blocks.20.ff.net.2.bias', 'transformer_blocks.21.norm1.bias', 'transformer_blocks.11.ff.net.2.weight', 'transformer_blocks.23.attn1.to_q.weight', 'transformer_blocks.12.attn2.to_v.weight', 'transformer_blocks.11.attn2.to_v.weight', 'transformer_blocks.20.ff.net.0.proj.bias', 'transformer_blocks.15.norm3.bias', 'transformer_blocks.17.norm1.bias', 'transformer_blocks.12.attn1.to_k.weight', 'transformer_blocks.13.ff.net.2.bias', 'transformer_blocks.14.attn2.to_q.weight', 'transformer_blocks.16.attn1.to_v.weight', 'transformer_blocks.18.norm3.bias', 'transformer_blocks.17.attn1.to_v.weight', 'transformer_blocks.15.norm2.bias', 'transformer_blocks.11.norm1.bias', 'transformer_blocks.23.attn1.to_v.weight', 'transformer_blocks.18.ff.net.2.bias', 'transformer_blocks.11.norm2.bias', 'transformer_blocks.17.norm2.weight', 'transformer_blocks.14.norm2.weight', 'transformer_blocks.10.attn1.to_v.weight', 'transformer_blocks.13.attn2.to_q.weight', 'transformer_blocks.12.norm3.bias', 'transformer_blocks.14.norm2.bias', 'transformer_blocks.15.attn2.to_out.0.weight', 'transformer_blocks.11.norm3.bias', 'transformer_blocks.21.ff.net.2.bias', 'transformer_blocks.19.norm1.bias', 'transformer_blocks.12.ff.net.0.proj.weight', 'transformer_blocks.15.ff.net.0.proj.weight', 'transformer_blocks.12.ff.net.0.proj.bias', 'transformer_blocks.19.attn1.to_k.weight', 'transformer_blocks.21.attn2.to_q.weight', 'transformer_blocks.19.norm3.weight', 'transformer_blocks.18.norm2.bias', 'transformer_blocks.12.attn1.to_out.0.weight', 'transformer_blocks.17.norm1.weight', 'global_proj.0.weight', 'transformer_blocks.15.norm2.weight', 'transformer_blocks.17.ff.net.0.proj.bias', 'transformer_blocks.13.ff.net.2.weight', 'transformer_blocks.15.ff.net.2.weight', 'transformer_blocks.18.attn1.to_v.weight', 'transformer_blocks.12.attn1.to_q.weight', 'transformer_blocks.12.attn2.to_q.weight', 'transformer_blocks.14.norm1.bias', 'transformer_blocks.12.attn2.to_out.0.weight', 'transformer_blocks.17.attn2.to_v.weight', 'transformer_blocks.17.attn2.to_q.weight', 'transformer_blocks.15.attn1.to_out.0.weight', 'transformer_blocks.18.norm1.weight', 'transformer_blocks.14.norm3.weight', 'global_proj.2.weight', 'transformer_blocks.23.norm2.weight', 'transformer_blocks.21.attn2.to_v.weight', 'transformer_blocks.21.ff.net.0.proj.bias', 'transformer_blocks.11.ff.net.0.proj.weight', 'transformer_blocks.18.attn2.to_v.weight', 'transformer_blocks.11.attn1.to_v.weight', 'transformer_blocks.22.norm1.bias', 'transformer_blocks.16.norm2.weight', 'transformer_blocks.20.attn2.to_v.weight', 'transformer_blocks.10.norm1.weight', 'transformer_blocks.18.ff.net.2.weight', 'transformer_blocks.13.norm1.bias', 'transformer_blocks.14.ff.net.0.proj.bias', 'transformer_blocks.20.ff.net.2.weight', 'transformer_blocks.10.attn2.to_q.weight', 'transformer_blocks.22.attn2.to_k.weight']\n",
      "- This IS expected if you are initializing StableAudioDiTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing StableAudioDiTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of StableAudioDiTModel were not initialized from the model checkpoint at /home/chengxin/chengxin/vasflow/model/stable_audio/ckpt/transformer_text and are newly initialized because the shapes did not match:\n",
      "- postprocess_conv.weight: found shape torch.Size([64, 64, 1]) in the checkpoint and torch.Size([161, 161, 1]) in the model instantiated\n",
      "- preprocess_conv.weight: found shape torch.Size([64, 64, 1]) in the checkpoint and torch.Size([161, 161, 1]) in the model instantiated\n",
      "- proj_in.weight: found shape torch.Size([1536, 64]) in the checkpoint and torch.Size([1536, 161]) in the model instantiated\n",
      "- proj_out.weight: found shape torch.Size([64, 1536]) in the checkpoint and torch.Size([161, 1536]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Restored vaflow ckpt from /home/chengxin/chengxin/vasflow/log/2025_07_14-15_32_04-vaflow_sda_dit_noise_text_mixas_mel_novide_randdropspkebd/ckpt/epoch=0019-step=1.26e+04.ckpt\n",
      "=> Deleting key dp.phone_encoder.positial_embedding_q.pe from state_dict.\n",
      "=> Deleting key dp.hubert_encoder.positial_embedding_q.pe from state_dict.\n",
      "=> Deleting key dp.attn_decoder.positial_embedding_q.pe from state_dict.\n",
      "=> Deleting key dp.attn_decoder.positial_embedding_kv.pe from state_dict.\n",
      "=> Restored vaflow ckpt from /home/chengxin/chengxin/vasflow/log/2025_07_13-03_48_53-dp_tts_Lattn/ckpt/epoch=0319-step=1.31e+05.ckpt\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from worker.vaflow_sda_dit_noise_text_mel import VAFlow\n",
    "from worker.dp_tts import DurationPredictor\n",
    "from util.inference import infer_videos\n",
    "\n",
    "\n",
    "device = 'cuda:0'\n",
    "with open('/home/chengxin/chengxin/vasflow/config/mode_vaflow.yaml', 'r') as f:\n",
    "    vaflow_config = yaml.safe_load(f)\n",
    "vaflow = VAFlow(**vaflow_config).to(device)\n",
    "\n",
    "\n",
    "with open('/home/chengxin/chengxin/vasflow/config/model_dp.yaml', 'r') as f:\n",
    "    dp_config = yaml.safe_load(f)\n",
    "dp = DurationPredictor(**dp_config).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df674f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 250, 23])\n",
      "torch.Size([1, 23]) torch.Size([24]) torch.Size([1, 250, 23]) torch.Size([250, 24])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# keyboard_women     epoch0007 0019\n",
    "va_paths = ['/home/chengxin/chengxin/vasflow/test/keyboard_woman.mp4']\n",
    "text_seq = ['Hi, and welcome to the channel']\n",
    "speech_start_durations = [4.5]   \n",
    "video_process_config = {'duration':10.0, 'resize_input_size': [224, 224], 'target_sampling_rate': 10, 'raw_duration_min_threshold':0.05}\n",
    "\n",
    "# 可以没有。speech_durations和avhubert_feature_paths只需要有一个\n",
    "speech_durations = [2]\n",
    "avhubert_feature_paths = None #['/nfs-04/yuyue/visualtts_datasets/lrs2/lrs2_for_espnet/preprocessed_data/lip_feature/6330311066473698535_27418_00000.npy']\n",
    "ref_audio_ebd_paths = None #['/home/chengxin/chengxin/Dataset_Sound/LRS2/speaker_emb/6330311066473698535_27418/6330311066473698535_27418_00000.npy']\n",
    "\n",
    "\n",
    "\n",
    "infer_videos(\n",
    "    vaflow_model = vaflow, \n",
    "    dp_model = dp,\n",
    "    save_path='/home/chengxin/chengxin/vasflow/test', \n",
    "    mp4_paths=va_paths, \n",
    "    video_process_config = video_process_config, \n",
    "    text_seq = text_seq,                              \n",
    "    avhubert_feature_paths = avhubert_feature_paths,  # 抽取avhubert_feature的代码没有实现\n",
    "    speech_durations=speech_durations,\n",
    "    speech_start_durations=speech_start_durations,\n",
    "    ref_audio_ebd_paths = ref_audio_ebd_paths,        # 抽取ref_audio_ebd的代码没有实现\n",
    "    num_samples_per_prompt = 10,\n",
    "    guidance_scale = 3,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea360bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 250, 27])\n",
      "torch.Size([1, 27]) torch.Size([28]) torch.Size([1, 250, 27]) torch.Size([250, 28])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# car_police      epoch0007 0019\n",
    "va_paths = ['/home/chengxin/chengxin/vasflow/test/car_police.mp4']\n",
    "text_seq = ['We get in there, I want no bullshit.']\n",
    "speech_start_durations = [2.5]   \n",
    "video_process_config = {'duration':10.0, 'resize_input_size': [224, 224], 'target_sampling_rate': 10, 'raw_duration_min_threshold':0.05}\n",
    "\n",
    "# 可以没有。speech_durations和avhubert_feature_paths只需要有一个\n",
    "speech_durations = [2.5]\n",
    "avhubert_feature_paths = None #['/nfs-04/yuyue/visualtts_datasets/lrs2/lrs2_for_espnet/preprocessed_data/lip_feature/6330311066473698535_27418_00000.npy']\n",
    "ref_audio_ebd_paths = None #['/home/chengxin/chengxin/Dataset_Sound/LRS2/speaker_emb/6330311066473698535_27418/6330311066473698535_27418_00000.npy']\n",
    "\n",
    "\n",
    "\n",
    "infer_videos(\n",
    "    vaflow_model = vaflow, \n",
    "    dp_model = dp,\n",
    "    save_path='/home/chengxin/chengxin/vasflow/test', \n",
    "    mp4_paths=va_paths, \n",
    "    video_process_config = video_process_config, \n",
    "    text_seq = text_seq,                              \n",
    "    avhubert_feature_paths = avhubert_feature_paths,  # 抽取avhubert_feature的代码没有实现\n",
    "    speech_durations=speech_durations,\n",
    "    speech_start_durations=speech_start_durations,\n",
    "    ref_audio_ebd_paths = ref_audio_ebd_paths,        # 抽取ref_audio_ebd的代码没有实现\n",
    "    device=device,\n",
    "    num_samples_per_prompt = 10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87e701e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 250, 19])\n",
      "torch.Size([1, 19]) torch.Size([20]) torch.Size([1, 250, 19]) torch.Size([250, 20])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# man_zombie    epoch0019\n",
    "va_paths = ['/home/chengxin/chengxin/vasflow/test/man_zombie2.mp4']\n",
    "text_seq = ['Eat led, zombies scum.']\n",
    "speech_start_durations = [0.5]   \n",
    "video_process_config = {'duration':10.0, 'resize_input_size': [224, 224], 'target_sampling_rate': 10, 'raw_duration_min_threshold':0.05}\n",
    "\n",
    "# 可以没有。speech_durations和avhubert_feature_paths只需要有一个\n",
    "speech_durations = [1.8]\n",
    "avhubert_feature_paths = None #['/nfs-04/yuyue/visualtts_datasets/lrs2/lrs2_for_espnet/preprocessed_data/lip_feature/6330311066473698535_27418_00000.npy']\n",
    "ref_audio_ebd_paths = None #['/home/chengxin/chengxin/Dataset_Sound/LRS2/speaker_emb/6330311066473698535_27418/6330311066473698535_27418_00000.npy']\n",
    "\n",
    "\n",
    "\n",
    "infer_videos(\n",
    "    vaflow_model = vaflow, \n",
    "    dp_model = dp,\n",
    "    save_path='/home/chengxin/chengxin/vasflow/test', \n",
    "    mp4_paths=va_paths, \n",
    "    video_process_config = video_process_config, \n",
    "    text_seq = text_seq,                              \n",
    "    avhubert_feature_paths = avhubert_feature_paths,  # 抽取avhubert_feature的代码没有实现\n",
    "    speech_durations=speech_durations,\n",
    "    speech_start_durations=speech_start_durations,\n",
    "    ref_audio_ebd_paths = ref_audio_ebd_paths,        # 抽取ref_audio_ebd的代码没有实现\n",
    "    device=device,\n",
    "    guidance_scale = 4,\n",
    "    num_samples_per_prompt = 10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fadd4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a01cf46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /home/chengxin/chengxin/vasflow/test/keyboard_woman_00_replace_audio.mp4.\n",
      "MoviePy - Writing audio in keyboard_woman_00_replace_audioTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video /home/chengxin/chengxin/vasflow/test/keyboard_woman_00_replace_audio.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/chengxin/chengxin/vasflow/test/keyboard_woman_00_replace_audio.mp4\n",
      "Moviepy - Building video /home/chengxin/chengxin/vasflow/test/keyboard_woman_01_replace_audio.mp4.\n",
      "MoviePy - Writing audio in keyboard_woman_01_replace_audioTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video /home/chengxin/chengxin/vasflow/test/keyboard_woman_01_replace_audio.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/chengxin/chengxin/vasflow/test/keyboard_woman_01_replace_audio.mp4\n",
      "Moviepy - Building video /home/chengxin/chengxin/vasflow/test/keyboard_woman_02_replace_audio.mp4.\n",
      "MoviePy - Writing audio in keyboard_woman_02_replace_audioTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video /home/chengxin/chengxin/vasflow/test/keyboard_woman_02_replace_audio.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/chengxin/chengxin/vasflow/test/keyboard_woman_02_replace_audio.mp4\n",
      "Moviepy - Building video /home/chengxin/chengxin/vasflow/test/keyboard_woman_03_replace_audio.mp4.\n",
      "MoviePy - Writing audio in keyboard_woman_03_replace_audioTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video /home/chengxin/chengxin/vasflow/test/keyboard_woman_03_replace_audio.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/chengxin/chengxin/vasflow/test/keyboard_woman_03_replace_audio.mp4\n",
      "Moviepy - Building video /home/chengxin/chengxin/vasflow/test/keyboard_woman_04_replace_audio.mp4.\n",
      "MoviePy - Writing audio in keyboard_woman_04_replace_audioTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video /home/chengxin/chengxin/vasflow/test/keyboard_woman_04_replace_audio.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/chengxin/chengxin/vasflow/test/keyboard_woman_04_replace_audio.mp4\n",
      "Moviepy - Building video /home/chengxin/chengxin/vasflow/test/keyboard_woman_05_replace_audio.mp4.\n",
      "MoviePy - Writing audio in keyboard_woman_05_replace_audioTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video /home/chengxin/chengxin/vasflow/test/keyboard_woman_05_replace_audio.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/chengxin/chengxin/vasflow/test/keyboard_woman_05_replace_audio.mp4\n",
      "Moviepy - Building video /home/chengxin/chengxin/vasflow/test/keyboard_woman_06_replace_audio.mp4.\n",
      "MoviePy - Writing audio in keyboard_woman_06_replace_audioTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video /home/chengxin/chengxin/vasflow/test/keyboard_woman_06_replace_audio.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/chengxin/chengxin/vasflow/test/keyboard_woman_06_replace_audio.mp4\n",
      "Moviepy - Building video /home/chengxin/chengxin/vasflow/test/keyboard_woman_07_replace_audio.mp4.\n",
      "MoviePy - Writing audio in keyboard_woman_07_replace_audioTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video /home/chengxin/chengxin/vasflow/test/keyboard_woman_07_replace_audio.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/chengxin/chengxin/vasflow/test/keyboard_woman_07_replace_audio.mp4\n",
      "Moviepy - Building video /home/chengxin/chengxin/vasflow/test/keyboard_woman_08_replace_audio.mp4.\n",
      "MoviePy - Writing audio in keyboard_woman_08_replace_audioTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video /home/chengxin/chengxin/vasflow/test/keyboard_woman_08_replace_audio.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/chengxin/chengxin/vasflow/test/keyboard_woman_08_replace_audio.mp4\n",
      "Moviepy - Building video /home/chengxin/chengxin/vasflow/test/keyboard_woman_09_replace_audio.mp4.\n",
      "MoviePy - Writing audio in keyboard_woman_09_replace_audioTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video /home/chengxin/chengxin/vasflow/test/keyboard_woman_09_replace_audio.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/chengxin/chengxin/vasflow/test/keyboard_woman_09_replace_audio.mp4\n"
     ]
    }
   ],
   "source": [
    "import moviepy.editor as mpy\n",
    "\n",
    "# 输入视频和音频路径\n",
    "name = 'keyboard_woman'\n",
    "for idx in range(10):\n",
    "    input_video = f'/home/chengxin/chengxin/vasflow/test/{name}.mp4'\n",
    "    input_audio = f'/home/chengxin/chengxin/vasflow/test/{name}_{idx:02d}.wav'  # 假设wav和npy同名同目录\n",
    "    output_video = input_audio.replace('.wav', '_replace_audio.mp4')\n",
    "\n",
    "    # 加载视频和音频\n",
    "    video_clip = mpy.VideoFileClip(input_video)\n",
    "    audio_clip = mpy.AudioFileClip(input_audio)\n",
    "    audio_clip = audio_clip.subclip(0, video_clip.duration)\n",
    "\n",
    "\n",
    "    # 替换音频\n",
    "    video_with_new_audio = video_clip.set_audio(audio_clip)\n",
    "    video_with_new_audio.write_videofile(output_video, codec='libx264', audio_codec='aac')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee89ca88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "name = 'us_wm_veo_3_a-medium-shot-frames-an-old-sailor-his-knitted-blue-sailor-hat_LLpQrIw'\n",
    "for file in glob(f\"/home/chengxin/chengxin/vasflow/test/{name}*\"):\n",
    "    os.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b13bbce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da299536",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# va_paths = ['/home/chengxin/chengxin/vasflow/test/us_wm_veo_3_a-medium-shot-frames-an-old-sailor-his-knitted-blue-sailor-hat_LLpQrIw.mp4']\n",
    "# text_seq = ['This ocean, is a force, a wild, untamed might. .  And she commands you all, with every breaking light.']\n",
    "# speech_start_durations = [0]   \n",
    "# video_process_config = {'duration':10.0, 'resize_input_size': [224, 224], 'target_sampling_rate': 10, 'raw_duration_min_threshold':0.05}\n",
    "\n",
    "# # 可以没有。speech_durations和avhubert_feature_paths只需要有一个\n",
    "# speech_durations = [8]\n",
    "# avhubert_feature_pahs = None #['/nfs-04/yuyue/visualtts_datasets/lrs2/lrs2_for_espnet/preprocessed_data/lip_feature/6330311066473698535_27418_00000.npy']\n",
    "# ref_audio_ebd_paths = ['/home/chengxin/chengxin/Dataset_Sound/LRS2/speaker_emb/6330311066473698535_27418/6330311066473698535_27418_00000.npy']\n",
    "\n",
    "\n",
    "\n",
    "# infer_videos(\n",
    "#     vaflow_model = vaflow, \n",
    "#     dp_model = dp,\n",
    "#     save_path='/home/chengxin/chengxin/vasflow/test', \n",
    "#     mp4_paths=va_paths, \n",
    "#     video_process_config = video_process_config, \n",
    "#     text_seq = text_seq,                              \n",
    "#     avhubert_feature_paths = avhubert_feature_paths,  # 抽取avhubert_feature的代码没有实现\n",
    "#     speech_durations=speech_durations,\n",
    "#     speech_start_durations=speech_start_durations,\n",
    "#     ref_audio_ebd_paths = ref_audio_ebd_paths,        # 抽取ref_audio_ebd的代码没有实现\n",
    "#     device=device,\n",
    "#     num_samples_per_prompt = 10,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6957af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from util.get_dict import PhonemeTokenizer\n",
    "# # from util.inference import phonemes_to_token_ids\n",
    "# import torch\n",
    "\n",
    "# def text_to_token_ids(text_seq, config_path='/home/chengxin/chengxin/Dataset_Sound/MetaData/vaflow2_meta/meta/token_list.json'):\n",
    "#     tokenizer = PhonemeTokenizer(g2p_type=\"g2p_en_no_space\", non_linguistic_symbols=None,)\n",
    "#     with open(config_path, 'r') as f:\n",
    "#         phoneme2id = yaml.safe_load(f)\n",
    "#     token_ids = []\n",
    "\n",
    "#     for text in text_seq:\n",
    "#         phonemes = [\"<blank>\"] + tokenizer.text2tokens(text) + [\"<blank>\"]\n",
    "#         token_id = [phoneme2id.get(p, phoneme2id.get('<blank>', 0)) for p in phonemes]\n",
    "#         token_ids.append(torch.tensor(token_id, dtype=torch.long))\n",
    "#     return token_ids\n",
    "\n",
    "# text_seq = [\"I will give a try. Believe in me.\", \"Fuck me\", \"And For me the surprise was\"]\n",
    "# text_to_token_ids(text_seq=text_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e507da1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os, time, random, einops, wandb, inspect\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from tqdm import tqdm\n",
    "# from PIL import Image\n",
    "\n",
    "\n",
    "# import torch, torchaudio\n",
    "# import torch.nn as nn\n",
    "# import torchvision.io as tvio\n",
    "# import torchvision.transforms as transforms\n",
    "# from torchvision.transforms import InterpolationMode\n",
    "\n",
    "\n",
    "# import yaml\n",
    "# import decord\n",
    "# from worker.vaflow_sda_dit_noise_text_mel import VAFlow\n",
    "# from worker.dp_tts import DurationPredictor\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "# def maxPathSumWithPath(matrix):\n",
    "#     m, n = len(matrix), len(matrix[0])\n",
    "#     dp = [[float('-inf')] * n for _ in range(m)]\n",
    "#     prev = [[None] * n for _ in range(m)]  # 记录前驱方向\n",
    "    \n",
    "#     dp[0][0] = matrix[0][0]\n",
    "#     for j in range(1, n):\n",
    "#         dp[0][j] = dp[0][j-1] + matrix[0][j]\n",
    "#         prev[0][j] = 'left'\n",
    "#     for i in range(1, m):\n",
    "#         dp[i][0] = dp[i-1][0] + matrix[i][0]\n",
    "#         prev[i][0] = 'up'\n",
    "#     for i in range(1, m):\n",
    "#         for j in range(1, n):\n",
    "#             max_val = float('-inf')\n",
    "#             direction = None\n",
    "            \n",
    "#             if i > 0 and dp[i-1][j] > max_val:\n",
    "#                 max_val = dp[i-1][j]\n",
    "#                 direction = 'up'\n",
    "#             if j > 0 and dp[i][j-1] > max_val:\n",
    "#                 max_val = dp[i][j-1]\n",
    "#                 direction = 'left'\n",
    "#             if i > 0 and j > 0 and dp[i-1][j-1] > max_val:\n",
    "#                 max_val = dp[i-1][j-1]\n",
    "#                 direction = 'diag'\n",
    "            \n",
    "#             dp[i][j] = max_val + matrix[i][j]\n",
    "#             prev[i][j] = direction\n",
    "    \n",
    "\n",
    "#     path = []\n",
    "#     i, j = m-1, n-1\n",
    "#     while i >= 0 and j >= 0:\n",
    "#         path.append((i, j))\n",
    "#         if prev[i][j] == 'up':\n",
    "#             i -= 1\n",
    "#         elif prev[i][j] == 'left':\n",
    "#             j -= 1\n",
    "#         elif prev[i][j] == 'diag':\n",
    "#             i -= 1\n",
    "#             j -= 1\n",
    "#         else:\n",
    "#             break \n",
    "    \n",
    "#     path.reverse()  \n",
    "#     return dp[m-1][n-1], path\n",
    "\n",
    "\n",
    "\n",
    "# def get_video_frames(\n",
    "#     va_path,\n",
    "#     video_process_config={'duration':10.0, 'resize_input_size': [224, 224], 'target_sampling_rate': 10, 'raw_duration_min_threshold':0.05},\n",
    "#     backend=\"decord\"\n",
    "# ):\n",
    "#     def check_and_drop(int_list, min_value, max_value):\n",
    "#         return [x for x in int_list if min_value <= x <= max_value]\n",
    "#     video_fram_h, video_frame_w = video_process_config['resize_input_size']\n",
    "#     clip_transform = transforms.Compose([\n",
    "#             transforms.Resize((video_fram_h, video_frame_w), interpolation=InterpolationMode.BICUBIC),\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "#     ])\n",
    "#     video_process_config['target_frame_length'] = int(video_process_config['duration'] * video_process_config['target_sampling_rate'])\n",
    "\n",
    "#     if backend == \"torchvision\":\n",
    "#         try:\n",
    "#             frame_data, _, meta = tvio.read_video(va_path, pts_unit=\"sec\", output_format=\"THWC\")\n",
    "#             video_raw_fps = meta[\"video_fps\"]\n",
    "#         except BaseException as e:\n",
    "#             raise e\n",
    "#         video_raw_frame_num = len(frame_data)\n",
    "#         video_raw_duration = video_raw_frame_num / video_raw_fps\n",
    "#         if video_raw_duration <= video_process_config['raw_duration_min_threshold']:\n",
    "#             raise RuntimeError(f\"Video duration {video_raw_duration} is too short, less than {video_process_config['raw_duration_min_threshold']}.\")\n",
    "#         video_sampled_frame_ids = [\n",
    "#             round(i * video_raw_fps / video_process_config['target_sampling_rate'])\n",
    "#             for i in range(video_process_config['target_frame_length'])\n",
    "#         ]\n",
    "#         video_sampled_frame_ids = check_and_drop(video_sampled_frame_ids, 0, video_raw_frame_num-1)\n",
    "#         sampled_frames = frame_data[video_sampled_frame_ids, :, :, :]\n",
    "#         if sampled_frames.dtype != torch.uint8:\n",
    "#             if sampled_frames.max() <= 1.0:\n",
    "#                 sampled_frames = (sampled_frames * 255).byte()\n",
    "#             else:\n",
    "#                 sampled_frames = sampled_frames.byte()\n",
    "#         sampled_frames = [Image.fromarray(frame.numpy()) for frame in sampled_frames]\n",
    "#     elif backend == \"decord\":\n",
    "#         try:\n",
    "#             decord_vr = decord.VideoReader(va_path, num_threads=1, ctx=decord.cpu(0))\n",
    "#         except BaseException as e:\n",
    "#             raise e\n",
    "#         video_raw_frame_num = len(decord_vr)\n",
    "#         video_raw_fps = decord_vr.get_avg_fps()\n",
    "#         video_raw_duration = video_raw_frame_num / video_raw_fps\n",
    "#         if video_raw_duration <= video_process_config['raw_duration_min_threshold']:\n",
    "#             raise RuntimeError(f\"Video duration {video_raw_duration} is too short, less than {video_process_config['raw_duration_min_threshold']}.\")\n",
    "            \n",
    "#         # Sample frames with target sampling rate, \n",
    "#         # NOTE Implemention of `FPS`` here is just selecting frames in raw 30fps video frames\n",
    "#         video_sampled_frame_ids = [ round(i * video_raw_fps / video_process_config['target_sampling_rate'])\n",
    "#                                     for i in range(video_process_config['target_frame_length']) ]\n",
    "#         video_sampled_frame_ids = check_and_drop(video_sampled_frame_ids, 0, video_raw_frame_num-1)\n",
    "#         sampled_frames = decord_vr.get_batch(video_sampled_frame_ids).asnumpy()\n",
    "            \n",
    "#         if sampled_frames.dtype != np.uint8:\n",
    "#             if sampled_frames.max() <= 1.0:\n",
    "#                 sampled_frames = (sampled_frames * 255).astype(np.uint8)\n",
    "#             else:\n",
    "#                 sampled_frames = sampled_frames.astype(np.uint8)\n",
    "            \n",
    "#         sampled_frames = [Image.fromarray(frame) for frame in sampled_frames]\n",
    "\n",
    "#     else:\n",
    "#         raise NotImplementedError(f\"Backend `{backend}` is not implemented.\")\n",
    "\n",
    "#     ''' Process frames '''\n",
    "#     p = video_process_config['target_frame_length'] - len(sampled_frames)\n",
    "#     if p > 0:\n",
    "#         w, h = sampled_frames[0].size  \n",
    "#         blank_pil = Image.new('RGB', (h, w), (0, 0, 0))\n",
    "#         sampled_frames = sampled_frames + [blank_pil] * p\n",
    "#     else:\n",
    "#         sampled_frames = sampled_frames[0 : video_process_config['target_frame_length']]\n",
    "#     assert len(sampled_frames) == video_process_config['target_frame_length'], f\"Sampled frames length {len(sampled_frames)} is not equal to target length {self.video_target_frame_len}.\"\n",
    "#     # Transform and stack\n",
    "#     sampled_frames = [clip_transform(sampled_frame) for sampled_frame in sampled_frames ]\n",
    "#     processed_frames = torch.stack(sampled_frames, dim=0)\n",
    "#     return processed_frames\n",
    "\n",
    "\n",
    "\n",
    "# def phonemes_to_token_ids(phoneme_seq, config_path='/home/chengxin/chengxin/Dataset_Sound/MetaData/vaflow2_meta/meta/token_list.json'):\n",
    "#     with open(config_path, 'r') as f:\n",
    "#         phoneme2id = yaml.safe_load(f)\n",
    "#     token_ids = [phoneme2id.get(p, phoneme2id.get('<unk>', 0)) for p in phoneme_seq]\n",
    "#     return torch.tensor(token_ids, dtype=torch.long)\n",
    "\n",
    "\n",
    "\n",
    "# def pad_sequence(sequences, padding_value=0, max_length=None, pad_type='right', target_dim=None):\n",
    "#     if not sequences:\n",
    "#         raise ValueError(\"sequences should not be empty\")\n",
    "\n",
    "#     # Determine which dimension to pad (default: 0)\n",
    "#     if target_dim is None:\n",
    "#         target_dim = 0\n",
    "\n",
    "#     # Find max length\n",
    "#     lengths = [s.shape[target_dim] for s in sequences]\n",
    "#     pad_len = max(lengths) if max_length is None else max_length\n",
    "\n",
    "\n",
    "#     # Pad each tensor\n",
    "#     padded = []\n",
    "#     for s in sequences:\n",
    "#         pad_size = pad_len - s.shape[target_dim]\n",
    "#         if pad_size < 0:\n",
    "#             raise ValueError(\"Some sequence is longer than pad_len\")\n",
    "#         pad_shape = [0] * (2 * s.dim())\n",
    "#         pad_shape[2 * (s.dim() - target_dim) - 1] = pad_size  # pad right\n",
    "#         if pad_type == 'left':\n",
    "#             pad_shape[2 * (s.dim() - target_dim) - 2] = pad_size  # pad left\n",
    "#             pad_shape[2 * (s.dim() - target_dim) - 1] = 0\n",
    "#         s_padded = torch.nn.functional.pad(s, pad_shape, mode='constant', value=padding_value)\n",
    "#         padded.append(s_padded)\n",
    "#     return torch.stack(padded, dim=0)\n",
    "\n",
    "\n",
    "\n",
    "# def adjust_speech(speech_start_duration, phone_id, duration_matrix, device):\n",
    "#     # phone_id [phone_length]    duration_matrix [250, phone_length]\n",
    "#     # phone_id = torch.cat((torch.tensor([59]).to(phone_id), phone_id))\n",
    "#     if speech_start_duration == 0:\n",
    "#         return torch.cat((torch.tensor([0], dtype=torch.int).to(phone_id), phone_id)), \\\n",
    "#                 torch.cat([duration_matrix, torch.zeros([250 ,1]).to(duration_matrix)], dim = -1)\n",
    "    \n",
    "    \n",
    "#     assert phone_id[0] == 59\n",
    "#     phone_id = torch.cat((torch.tensor([0], dtype=torch.int).to(phone_id), phone_id))            # [1 + phone_length]\n",
    "#     silence_length = int(round(speech_start_duration*25, 0))\n",
    "#     duration_matrix_new = torch.zeros([silence_length, phone_id.shape[-1]], device=device)     # [append_dur_length, 1 + phone_length]\n",
    "#     duration_matrix_new[:, 0] = 1\n",
    "\n",
    "#     assert duration_matrix[-silence_length].sum() == 0\n",
    "#     duration_matrix = duration_matrix[:-silence_length]                              # [250 - append_dur_length, phone_length]\n",
    "#     duration_matrix = F.pad(duration_matrix, (1, 0), mode='constant', value=0)       # [250 - append_dur_length, 1 + phone_length]\n",
    "#     duration_matrix_new = torch.cat((duration_matrix_new, duration_matrix), dim=0)   # [250, 1 + phone_length]\n",
    "#     return phone_id, duration_matrix_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fe7a74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import numpy as np\n",
    "\n",
    "# def phonemes_to_token_ids(phoneme_seq, config_path='/home/chengxin/chengxin/Dataset_Sound/MetaData/vaflow2_meta/meta/token_list.json'):\n",
    "#     with open(config_path, 'r') as f:\n",
    "#         phoneme2id = yaml.safe_load(f)\n",
    "#     token_ids = [phoneme2id.get(p, phoneme2id.get('<unk>', 0)) for p in phoneme_seq]\n",
    "#     return torch.tensor(token_ids, dtype=torch.long)\n",
    "\n",
    "\n",
    "# def pad_sequence(sequences, padding_value=0, max_length=None, pad_type='right', target_dim=None):\n",
    "#     if not sequences:\n",
    "#         raise ValueError(\"sequences should not be empty\")\n",
    "\n",
    "#     # Determine which dimension to pad (default: 0)\n",
    "#     if target_dim is None:\n",
    "#         target_dim = 0\n",
    "\n",
    "#     # Find max length\n",
    "#     lengths = [s.shape[target_dim] for s in sequences]\n",
    "#     pad_len = max(lengths) if max_length is None else max_length\n",
    "\n",
    "\n",
    "#     # Pad each tensor\n",
    "#     padded = []\n",
    "#     for s in sequences:\n",
    "#         pad_size = pad_len - s.shape[target_dim]\n",
    "#         if pad_size < 0:\n",
    "#             raise ValueError(\"Some sequence is longer than pad_len\")\n",
    "#         pad_shape = [0] * (2 * s.dim())\n",
    "#         pad_shape[2 * (s.dim() - target_dim) - 1] = pad_size  # pad right\n",
    "#         if pad_type == 'left':\n",
    "#             pad_shape[2 * (s.dim() - target_dim) - 2] = pad_size  # pad left\n",
    "#             pad_shape[2 * (s.dim() - target_dim) - 1] = 0\n",
    "#         s_padded = torch.nn.functional.pad(s, pad_shape, mode='constant', value=padding_value)\n",
    "#         padded.append(s_padded)\n",
    "#     return torch.stack(padded, dim=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # phoneme_seqs = ['<blank> S OW1 L EH1 T S T AO1 K AH0 B AW1 T AH0 B Z AO1 R P SH AH0 N AH1 V L AY1 T <blank>', '<blank> S OW1 L EH1 T S T AO1 K AH0 B AW1 T AH0 B Z AO1 R P SH AH0 N AH1 V T <blank>']\n",
    "# # phoneme_seqs = [phoneme_seq.split(\" \") for phoneme_seq in phoneme_seqs]\n",
    "# # avhubert_feature_paths = None\n",
    "# # speech_durations=[1.6, 1.6]\n",
    "# # device=\"cuda:0\"\n",
    "\n",
    "# # # Convert phoneme sequences to token ids and pad\n",
    "# # token_ids = [phonemes_to_token_ids(seq) for seq in phoneme_seqs]\n",
    "# # phone_id = pad_sequence(token_ids, padding_value=0).to(device)  # [B, max_seq_len]\n",
    "# # print(phone_id.shape)\n",
    "# # phone_length = torch.tensor([len(seq) for seq in token_ids], dtype=torch.long, device=device)  # [B]\n",
    "\n",
    "# # # Prepare avhubert features and pad\n",
    "# # avhubert_list = []\n",
    "# # avhubert_length = []\n",
    "# # if avhubert_feature_paths is not None:\n",
    "# #     for path in avhubert_feature_paths:\n",
    "# #         feat = torch.tensor(np.load(path), device=device)\n",
    "# #         avhubert_list.append(feat)\n",
    "# #         avhubert_length.append(feat.shape[0])\n",
    "# # else:\n",
    "# #     assert speech_durations is not None\n",
    "# #     for dur in speech_durations:\n",
    "# #         feat = torch.ones(int(25 * dur), 1024, device=device)\n",
    "# #         avhubert_list.append(feat)\n",
    "# #         avhubert_length.append(feat.shape[0])\n",
    "# # avhubert = pad_sequence(avhubert_list, padding_value=0.0, max_length = 250)  # [B, max_avhubert_len, 1024]\n",
    "# # avhubert_length = torch.tensor(avhubert_length, dtype=torch.long, device=device)  # [B]\n",
    "\n",
    "# # batch = {\n",
    "# #     \"video_id\": None,\n",
    "# #     \"duration_matrix\": None,\n",
    "# #     \"duration_span\": None,\n",
    "# #     \"avhubert\": avhubert,\n",
    "# #     \"avhubert_length\": avhubert_length,\n",
    "# #     \"phone_id\": phone_id,\n",
    "# #     \"phone_length\": phone_length,\n",
    "# # }\n",
    "\n",
    "\n",
    "# # attns = dp.predict_step(batch)\n",
    "# # attns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d0ebd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# va_paths = ['/home/chengxin/chengxin/Dataset_Sound/LRS2/video_25fps/6330311066473698535_27418/6330311066473698535_27418_00000.mp4']\n",
    "# # va_paths = ['/home/chengxin/chengxin/Dataset_Sound/VGGSound/dataset/test/10/__2MwJ2uHu0_000004.mp4']\n",
    "# text_seq = ['And, For me ,the surprise was']\n",
    "# speech_start_durations = None # [2]   \n",
    "# video_process_config = {'duration':10.0, 'resize_input_size': [224, 224], 'target_sampling_rate': 10, 'raw_duration_min_threshold':0.05}\n",
    "\n",
    "# # 可以没有。speech_durations和avhubert_feature_paths只需要有一个\n",
    "# # mix generation的效果还没测\n",
    "# speech_durations = [2.5]\n",
    "# avhubert_feature_paths = ['/nfs-04/yuyue/visualtts_datasets/lrs2/lrs2_for_espnet/preprocessed_data/lip_feature/6330311066473698535_27418_00000.npy']\n",
    "# ref_audio_ebd_paths = ['/home/chengxin/chengxin/Dataset_Sound/LRS2/speaker_emb/6330311066473698535_27418/6330311066473698535_27418_00000.npy']\n",
    "\n",
    "\n",
    "\n",
    "# infer_videos(\n",
    "#     vaflow_model = vaflow, \n",
    "#     dp_model = dp,\n",
    "#     save_path='/home/chengxin/chengxin/vasflow/test', \n",
    "#     mp4_paths=va_paths, \n",
    "#     video_process_config = video_process_config, \n",
    "#     text_seq = text_seq,                              \n",
    "#     avhubert_feature_paths = avhubert_feature_paths,  # 抽取avhubert_feature的代码没有实现\n",
    "#     speech_durations=speech_durations,\n",
    "#     speech_start_durations=speech_start_durations,\n",
    "#     ref_audio_ebd_paths = ref_audio_ebd_paths,        # 抽取ref_audio_ebd的代码没有实现\n",
    "#     device=device\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vaflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
